# Citibike Data Prediction

Author: Sambhav Shrestha


## Importing the libraries

```{r}
library(tidyverse)
library(scales)
library(modelr)
library(Holidays)

```


## Load the dataset
```{r}
trips <- read_tsv('trips_per_day.tsv')
```

## Data visualization
```{r}
# visualize the graph between each independent variable with num_trips to analyze the data
trips %>%
  ggplot(aes(x = tmin, y = num_trips)) +
  geom_point()

```

## Splitting the data (Train and Test split)

```{r}
set.seed(50)

num_days <- nrow(trips)
train_frac <- 0.9
num_train <- floor(num_days * train_frac)

# sample certain fraction of random numbers between 1 and total rows of dataset
sample_data <- sample(1:num_days, num_train, replace = FALSE)

# training data (further needs to undergo cross validation)
train <- trips[sample_data, ]

# testing data
test <- trips[-sample_data, ]

```


## Cross Validataion

```{r}
# no of k folds
num_folds = 10

# mutate the training dataset to include fold numbers
train <- train %>%
  mutate(fold = (row_number() %% num_folds) + 1)


head(train)
```

## Model Testing for different degrees for each independent variable

```{r}
# testing the model for 8 polynomial degrees 
N <- 1:8

# create a vector for listing the errors
avg_training_err = c()
avg_validation_err = c()
se_validation_err = c()

# loop through each polynomial degrees
for (n in N) {
  
  # do 10 fold cross validation within each 
  validation_err = c()
  train_err = c()
  
  for (f in 1:num_folds) {
    
    # fit on training data
    train_data <- filter(train, fold != f)
    model <- lm(num_trips ~ poly(tmin, n, raw = T), data = train_data)
    
    # evaluate the training data
    train_err[f] = sqrt(mean((predict(model, train_data) - train_data$num_trips)^2))
    
    
    # evaluate the validation data
    validation_data <- filter(train, fold == f)
    validation_err[f] = sqrt(mean((predict(model, validation_data) - validation_data$num_trips)^2))
    
  }
  
  # compute avgerage training error
  avg_training_err[n] = mean(train_err)
  
  # compute average and standard validation error across folds
  avg_validation_err[n] = mean(validation_err)
  se_validation_err[n] = sd(validation_err)/sqrt(num_folds)
  
}

```

## plot the errors 

```{r}
plot_data <- data.frame(N, avg_training_err, avg_validation_err, se_validation_err)

plot_data %>%
  ggplot(aes(x = N, y = avg_validation_err)) +
  geom_pointrange(aes(ymin=avg_validation_err - se_validation_err,
                      ymax=avg_validation_err + se_validation_err,
                      color=avg_validation_err == min(avg_validation_err))) +
  geom_line(aes(color = 'Average Validation Error')) +
  geom_line(aes(y = avg_training_err, color = "Avg training error")) +
  labs(x = "Degrees", y = "Error Value", title ="Validation vs Training Error") +
  theme(legend.position = "none")

```


## Linear Regression Model

```{r}

# do 10 fold cross validation within each 
validation_err = c()
train_err = c()

for (f in 1:num_folds) {
  
  # fit on training data
  train_data <- filter(train, fold != f)
  model <- lm(num_trips ~  isHoliday(dates = ymd, type = "USFed") + factor(months(ymd)) + factor(weekdays(ymd)) + snwd + prcp + poly(tmin, 2, raw = T) + poly(tmax, 2, raw = T), data = train_data)
  
  # evaluate the training data
  train_err[f] = sqrt(mean((predict(model, train_data) - train_data$num_trips)^2))
  
  
  # evaluate the validation data using rmse
  validation_data <- filter(train, fold == f)
  validation_err[f] = sqrt(mean((predict(model, validation_data) - validation_data$num_trips)^2))
}

print(mean(validation_err))

summary(model)
  

```

## Data Plotting
```{r}
# select a random validation data from the training set
validation_data <- filter(train, fold == 2)
pred = predict(model, validation_data)

# plot the regression plot 
validation_data %>%
  mutate(pred = pred) %>%
  ggplot(aes(x = ymd)) +
  geom_point(aes(y = num_trips)) +
  geom_line(aes(y = pred, color = "prediction_line")) +
  labs(x = "Date", y = "No. of trips", title = "Linear Regression Plot")


# plot the actual vs predicted plot
validation_data %>%
  mutate(pred = pred) %>%
  ggplot(aes(x = ymd)) +
  geom_point(aes(y = pred, color = "prediction"), size = 3) +
  geom_point(aes(y = num_trips, color = "actual"), size = 3) +
  labs(x = "Date", y = "No. of trips", title = "Actual vs Predicted Plot")

```
## Saving the model

```{r}

save.image(file = "Sambhav_workspace.RData")

save(model, file = "sambhav_model.RData")

```


## Predicting the Test Set
```{r}
# Predict the test set
test_pred <- predict(model, test)

# find the rmse
test_error <- sqrt(mean((predict(model, test) - test$num_trips)^2))
print(test_error)

#plot the test set

# plot the regression plot 
test %>%
  mutate(pred = test_pred) %>%
  ggplot(aes(x = ymd)) +
  geom_point(aes(y = num_trips)) +
  geom_line(aes(y = pred, color = "prediction_line")) +
  labs(x = "Date", y = "No. of trips", title = "Linear Regression Plot")


# plot the actual vs predicted plot
test %>%
  mutate(pred = test_pred) %>%
  ggplot(aes(x = ymd)) +
  geom_point(aes(y = pred, color = "prediction"), size = 3) +
  geom_point(aes(y = num_trips, color = "actual"), size = 3) +
  labs(x = "Date", y = "No. of trips", title = "Actual vs Predicted Plot")


```

## Conclusion

#### Validation Error

When I used linear regression on only one feature, say tmin, I was getting the average validation error of about 5000-6000. 

After playing around quite a bit with different features and adding special cases like holidays, days of week, and months, I was able to obtain a consistent validation error of around 3000-4000, dropping the error rate by nearly half. In order to reduce the error, I was doing trial and error with different features based on the summary of the model (looking at Adjusted R squared, Pr(>|t|), and coefficients.)


#### Test Error

When I was finally satisfied with my model, I tested my model on the test set, and the model actually performed better than I thought it would. It also gave me a consistent error size of 3000-4000 just like the validation data. The plot actually made a reasonable regression line that captured most of the data.


#### Thoughts

When I viewed the plot of validation data, I wondered if my model has overfitted. After testing on different seed and different no. of folds, I got same error and same plot which increased my confidence in my model. Surprisingly, my model also performed really good on test set. Given how people bike around different times in a year and on different weather condition, this model might actually be able to predict no. of trips people take to a certain extent unless there is any extreme deviation from the training data.




